# -*- coding: utf-8 -*-
"""Book_Recomendation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RraBXAxkBAbVg37lO_jxMCHaoQFgPfP2

# **Proyek Akhir : Recomendation System**

## Read Data
"""

!pip install opendatasets
!pip install pandas

# download dataset
import opendatasets as od
import pandas as pd
od.download(
    "https://www.kaggle.com/datasets/arashnic/book-recommendation-dataset")

book_dataset = pd.read_csv('/content/book-recommendation-dataset/Books.csv')
rating_dataset = pd.read_csv('/content/book-recommendation-dataset/Ratings.csv')

# Cek jumlah dataset buku
book_dataset.shape

# Cek jumlah dataset rating
rating_dataset.shape

book_dataset = book_dataset[:10000]
rating_dataset=rating_dataset[:5000]

"""Berdasarkan jumlah dataset yang sangat banyak maka proyek ini hanya menggunakan 10000 data buku dan 5000 data rating

# Data Understanding
"""

# melihat dataset buku
book_dataset.head()

# melihat dataset rating
rating_dataset.head()

# melihat info dataset buku
book_dataset.info()

# melihat info dataset buku
rating_dataset.info()

"""Rename kolom dataset rating dan dataset buku karena nama kolom tersebut berisi spasi, dan huruf kapital sehingga perlu diperbaiki agar mudah digunakan. Berikut kodenya """

# rename kolom dataset rating
rating_dataset = rating_dataset.rename(columns={'Book-Rating': 'rating','User-ID':'user_id'})

# rename kolom dataset rating
book_dataset = book_dataset.rename(columns={'Book-Title': 'book_title','Book-Author':'book_author','Year-Of-Publication':
                                            'year_of_publication','Image-URL-S':'Image_URL_S','Image-URL-M':'Image_URL_M','Image-URL-L':'Image_URL_L'})

# Hapus kolom yang tidak dibutuhkan
book_dataset = book_dataset.drop(columns={'Image_URL_S','Image_URL_M','Image_URL_L'})

# melihat kembali dataset buku
book_dataset.head()

# melihat kembali dataset rating
rating_dataset.head()

!pip install plotly_express

# Top Author
import plotly_express as px
top_author_counts = book_dataset['book_author'].value_counts().reset_index()
top_author_counts.columns = ['value', 'count']
top_author_counts['value'] = top_author_counts['value']
top_author_counts = top_author_counts.sort_values('count')
fig = px.bar(top_author_counts.tail(50), x="count", y="value", title='Top Authors', orientation='h', color='value',
             width=1000, height=700)
fig.show()

# Top Book 

top_book_counts = book_dataset['book_title'].value_counts().reset_index()
top_book_counts.columns = ['value', 'count']
top_book_counts['value'] = top_book_counts['value']
top_book_counts = top_book_counts.sort_values('count')
fig = px.bar(top_book_counts.tail(20), x="count", y="value", title='Top Books', orientation='h', color='value',
             width=1000, height=700)
fig.show()

# memisahkan kumpulan data rating implisit dan eksplisit
ratings_new = rating_dataset[rating_dataset.ISBN.isin(book_dataset.ISBN)]
ratings_explicit = ratings_new[ratings_new['rating'] != 0]
ratings_implicit = ratings_new[ratings_new['rating'] == 0]

import matplotlib.pyplot as plt
import seaborn as sns

print(ratings_new.shape)
print(ratings_explicit.shape)
print(ratings_implicit.shape)
plt.style.use('fivethirtyeight')
plt.figure(figsize=(12, 8))
sns.countplot(data=ratings_explicit , x='rating', palette='rocket_r')

"""# Data Preparation"""

# cek missing value

book_dataset.isnull().sum()

# cek missing value

rating_dataset.isnull().sum()

"""Data Buku dan Rating tidak ada missing value"""

# Melihat buku yang di rate 10 
rating_dataset[rating_dataset.rating == max(rating_dataset.rating)]
best_booksId = rating_dataset.ISBN[rating_dataset.rating == max(rating_dataset.rating)]
best_booksId = list(dict.fromkeys(best_booksId))

best_books = []
for i in best_booksId:
    books_name = book_dataset.book_title[book_dataset.ISBN == i]
    best_books.append(books_name)
len(best_books)

"""Berdasarkan output di atas menunjukkan ada 300 buku yang di rate oleh pengguna."""

# Hapus data duplicate
book_dataset = book_dataset.dropna()
rating_dataset = rating_dataset.dropna()

rating_dataset = rating_dataset.drop_duplicates()
book_dataset = book_dataset.drop_duplicates()

"""Hapus data duplikat supaya dataset tetap memiliki integritas dan tidak berulang."""

# Mengonversi data series ‘ISBN’ menjadi dalam bentuk list
book_ISBN = book_dataset['ISBN'].tolist()
# Mengonversi data series ‘title’ menjadi dalam bentuk list
book_ISBN = book_dataset['ISBN'].tolist()
# Mengonversi data series ‘author’ menjadi dalam bentuk list
book_author = book_dataset['book_author'].tolist()
# Mengonversi data series ‘year_of_publication’ menjadi dalam bentuk list
book_year_of_publication = book_dataset['year_of_publication'].tolist()

book_title = book_dataset['book_title'].tolist()

print(len(book_ISBN))
print(len(book_author))
print(len(book_title))
print(len(book_year_of_publication))

# Membuat dictionary untuk data 
book = pd.DataFrame({
    'book_ISBN': book_ISBN,
    'book_title': book_title,
    'book_author': book_author,
    'book_year_of_publication': book_year_of_publication
})
book

"""Tahap berikutnya, kita akan membuat dictionary untuk menentukan pasangan key-value pada data book_title, book_author, ISBN dan book_year_of_publication yang telah kita siapkan sebelumnya.

## Content Based Filtering

Pemfilteran berbasis konten menangani masalah cold-start karena tidak memerlukan data interaksi apa pun untuk menghasilkan rekomendasi. Sebaliknya, ini melihat fitur pengguna (misalnya, usia, jenis kelamin, bahasa lisan, pekerjaan, dll.) dan fitur item (misalnya, genre film, tanggal rilis, pemeran).
"""

book_dataset.head(5)

from sklearn.feature_extraction.text import TfidfVectorizer
 
# Inisialisasi TfidfVectorizer
tf = TfidfVectorizer()
 
# Melakukan perhitungan idf pada data author
tf.fit(book['book_author']) 
# Mapping array dari fitur index integer ke fitur nama
tf.get_feature_names()

# Melakukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix = tf.fit_transform(book['book_author']) 
 
# Melihat ukuran matrix tfidf
tfidf_matrix.shape

"""Perhatikanlah, matriks yang kita miliki berukuran (10000, 5575). Nilai 10000 merupakan ukuran data dan 5575 merupakan matrik kategori penulis(author)"""

# Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
tfidf_matrix.todense()

# Membuat dataframe untuk melihat tf-idf matrix
# Kolom diisi dengan judul buku
# Baris diisi dengan nama penulis
 
pd.DataFrame(
    tfidf_matrix.todense(), 
    columns=tf.get_feature_names(),
    index=book.book_title).sample(10, axis=1,replace=True).sample(10, axis=0)

"""Pada tahap ini telah menghasilkan matriks yang menunjukkan korelasi antara judul buku dengan penulis. Sekarang, kita akan menghitung derajat kesamaan (similarity degree) antar penulis dengan teknik cosine similarity."""

from sklearn.metrics.pairwise import cosine_similarity
 
# Menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix) 
cosine_sim

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa judul buku
cosine_sim_df = pd.DataFrame(cosine_sim, index=book['book_title'], columns=book['book_title'])
print('Shape:', cosine_sim_df.shape)

"""Dengan cosine similarity, kita berhasil mengidentifikasi kesamaan antara satu buku dengan buku lainnya. Shape (10000, 10000) merupakan ukuran matriks similarity dari data yang kita miliki.

Selanjutnya, mari kita lihat matriks kesamaan setiapbbuku dengan menampilkan nama buku dalam 5 sampel kolom (axis = 1) dan 10 sampel baris (axis=0). Jalankan kode berikut.
"""

# Melihat similarity matrix pada setiap buku
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

def author_recommendations(i, M, items, k=5):

    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan    
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step) 
    ix = M.loc[:,i].to_numpy().argpartition(range(-1,-k,-1))

# Mengambil data dengan similarity terbesar dari index yang ada    
    closest = M.columns[ix[-1:-(k+2):-1]]

# Drop judulbuku agar judulbuku yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(i, errors='ignore')
    return pd.DataFrame(closest).merge(items).head(k)

"""Di sini, kita membuat fungsi author_recommendations dengan beberapa parameter sebagai berikut:

- i : Nama penulis (index kemiripan dataframe).
- M: Dataframe mengenai similarity yang telah kita definisikan sebelumnya.
- Items : Nama dan fitur yang digunakan untuk mendefinisikan kemiripan, dalam hal ini adalah ‘book_title’.
- k : Banyak rekomendasi yang ingin diberikan.
"""

book_dataset

books_that_have_been_read = "The Diaries of Adam and Eve"
book[book.book_title.eq(books_that_have_been_read)]

recommendations = author_recommendations(books_that_have_been_read, cosine_sim_df, book[['book_title', 'book_author']])

recommendations = recommendations.drop_duplicates()

recommendations

books_that_have_been_read_row = book_dataset[book_dataset.book_title == books_that_have_been_read]
books_that_have_been_read_author = books_that_have_been_read_row.iloc[0]["book_author"]

books_with_the_same_author = book_dataset[book_dataset.book_author == books_that_have_been_read_author].shape[0]
print(books_with_the_same_author)

Accuracy = (recommendations.shape[0]/books_with_the_same_author)*100
print("Accuracy of the model is {}%".format(Accuracy))

"""## Model Development dengan Collaborative Filtering

Content Based Filtering didasarkan pada konsep "homophily" — orang yang mirip menyukai hal yang serupa. Tujuannya adalah untuk memprediksi preferensi pengguna berdasarkan umpan balik dari pengguna serupa. Pada proyek ini hanya menggunakan dataset buku.
"""

# Membaca dataset
 
rating_dataset = rating_dataset
rating_dataset

# Mengubah userID menjadi list tanpa nilai yang sama
user_ids = rating_dataset['user_id'].unique().tolist()
print('list user_id: ', user_ids)
 
# Melakukan encoding userID
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded userID : ', user_to_user_encoded)
 
# Melakukan proses encoding angka ke ke userID
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke userID: ', user_encoded_to_user)

# Mengubah ISBN menjadi list tanpa nilai yang sama
book_ids = rating_dataset['ISBN'].unique().tolist()
# Melakukan proses encoding bookid
# Melakukan proses encoding angka ke bookid
book_to_book_encoded = {x: i for i, x in enumerate(book_ids)}
book_encoded_to_book = {i: x for i, x in enumerate(book_ids)}

# Mapping userID ke dataframe user
# Mapping ISBND ke dataframe BOOK
rating_dataset['user'] = rating_dataset['user_id'].map(user_to_user_encoded)
rating_dataset['book'] = rating_dataset['ISBN'].map(book_to_book_encoded)

import numpy as np
# Mendapatkan jumlah user
num_users = len(user_encoded_to_user)
print(num_users)
 
# Mendapatkan jumlah BUKU
num_book = len(book_encoded_to_book)
print(num_book)
 
# Mengubah rating menjadi nilai float
rating_dataset['rating'] = rating_dataset['rating'].values.astype(np.float32)
 
# Nilai minimum rating
# Nilai maksimal rating
min_rating = min(rating_dataset['rating'])
max_rating = max(rating_dataset['rating'])
 
print('Number of User: {}, Number of Buku: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_book, min_rating, max_rating))

"""**Membagi Data untuk Training dan Validasi**"""

# Mengacak dataset
rating_dataset = rating_dataset.sample(frac=1, random_state=42)
rating_dataset

# Membuat variabel x untuk mencocokkan data user dan resto menjadi satu value
x = rating_dataset[['user', 'book']].values
 
# Membuat variabel y untuk membuat rating dari hasil 
y = rating_dataset['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values
 
# Membagi menjadi 70% data train dan 20% data validasi
train_indices = int(0.70 * rating_dataset.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)
 
print(x, y)

"""**Proses Training**

Pada tahap ini, kita membuat class RecommenderNet dengan keras Model class.
"""

from tensorflow import keras
from tensorflow.keras import layers
import tensorflow as tf
class RecommenderNet(tf.keras.Model):
 
  # Insialisasi fungsi  
  def __init__(self, num_users, num_resto, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_resto = num_resto
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user

        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1)# layer embedding user bias
    self.resto_embedding = layers.Embedding(# layer embeddings buku 
        num_resto,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.resto_bias = layers.Embedding(num_resto, 1)#layer embedding book bias 
 
  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0])# memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0])# memanggil layer embedding 2
    resto_vector = self.resto_embedding(inputs[:, 1])# memanggil layer embedding 3
    resto_bias = self.resto_bias(inputs[:, 1])# memanggil layer embedding 4 
 
    dot_user_resto = tf.tensordot(user_vector, resto_vector, 2) 
 
    x = dot_user_resto + user_bias + resto_bias
    
    return tf.nn.sigmoid(x)# activation sigmoid

model = RecommenderNet(num_users, num_book, 50) # inisialisasi model
 
# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""Model ini menggunakan Binary Crossentropy untuk menghitung loss function, Adam (Adaptive Moment Estimation) sebagai optimizer, dan root mean squared error (RMSE) sebagai metrics evaluation. """

# Memulai training
 
history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 5,
    epochs = 20,
    validation_data = (x_val, y_val)
)

# Visualisasi Metrik
plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""**Mendapatkan Rekomendasi**"""

book_dataset

# Mengambil sample user
user_id = rating_dataset.user_id.sample(1).iloc[0]
books_have_been_read_by_user = rating_dataset[rating_dataset.user_id == user_id]
 
books_have_not_been_read_by_user = book_dataset[book_dataset['ISBN'].isin(books_have_been_read_by_user.ISBN.values)]['ISBN'] 
books_have_not_been_read_by_user = list(
    set(books_have_not_been_read_by_user)
    .intersection(set(book_to_book_encoded.keys()))
)
 
books_have_not_been_read_by_user = [[book_to_book_encoded.get(x)] for x in books_have_not_been_read_by_user]
user_encoder = user_to_user_encoded.get(user_id)
user_book_array = np.hstack(
    ([[user_encoder]] * len(books_have_not_been_read_by_user), books_have_not_been_read_by_user)
)

ratings = model.predict(user_book_array).flatten()
 
top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_book_ids = [
    book_encoded_to_book.get(books_have_not_been_read_by_user[x][0]) for x in top_ratings_indices
]
 
top_books_recommended = (
    books_have_been_read_by_user.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(5)
    .ISBN.values
)
 
books_row = book_dataset[book_dataset['ISBN'].isin(top_books_recommended)]
for row in books_row.itertuples():
    print(row.book_title, ':', row.book_author)
 
print('----' * 8)
print('Top 10 Book Recommendation for user: {}'.format(user_id))
print('----' * 8)
 
recommended_books = book_dataset[book_dataset['ISBN'].isin(recommended_book_ids)]
for row in recommended_books.itertuples():
    print(row.book_title, ':', row.book_author)

"""Sampai di tahap ini, proyek ini telah berhasil membuat sistem rekomendasi dengan dua teknik, yaitu Content based Filtering dan Collaborative Filtering. Sistem rekomendasi yang telah dibuat berhasil memberikan sejumlah rekomendasi buku yang sesuai dengan preferensi pengguna."""